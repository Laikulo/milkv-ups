diff -urpN --no-dereference -X diffgen.ignore test-tree/drivers/staging/android/ion/ion.c milky/drivers/staging/android/ion/ion.c
--- test-tree/drivers/staging/android/ion/ion.c	2024-05-24 09:36:29.343394612 -0400
+++ milky/drivers/staging/android/ion/ion.c	2024-05-21 05:22:27.000000000 -0400
@@ -23,19 +23,52 @@
 #include <linux/slab.h>
 #include <linux/uaccess.h>
 #include <linux/vmalloc.h>
+#include <trace/events/kmem.h>
+#ifdef CONFIG_COMPAT
+#include "compat_ion.h"
+#endif
 
 #include "ion.h"
-
+#ifdef CONFIG_ION_CVITEK
+extern void cvi_ion_create_debug_info(struct ion_heap *heap);
+#endif
 static struct ion_device *internal_dev;
 static int heap_id;
 
 /* this function should only be called while dev->lock is held */
+static void ion_buffer_add(struct ion_device *dev, struct ion_buffer *buffer)
+{
+	struct rb_node **p = &dev->buffers.rb_node;
+	struct rb_node *parent = NULL;
+	struct ion_buffer *entry;
+
+	while (*p) {
+		parent = *p;
+		entry = rb_entry(parent, struct ion_buffer, node);
+
+		if (buffer < entry) {
+			p = &(*p)->rb_left;
+		} else if (buffer > entry) {
+			p = &(*p)->rb_right;
+		} else {
+			pr_err("%s: buffer already found.", __func__);
+			BUG();
+		}
+	}
+
+	rb_link_node(&buffer->node, parent, p);
+	rb_insert_color(&buffer->node, &dev->buffers);
+}
+
+extern void cvi_ion_dump(struct ion_heap *heap);
+/* this function should only be called while dev->lock is held */
 static struct ion_buffer *ion_buffer_create(struct ion_heap *heap,
 					    struct ion_device *dev,
 					    unsigned long len,
 					    unsigned long flags)
 {
 	struct ion_buffer *buffer;
+	u64 pre_num_of_alloc_bytes;
 	int ret;
 
 	buffer = kzalloc(sizeof(*buffer), GFP_KERNEL);
@@ -66,6 +99,11 @@ static struct ion_buffer *ion_buffer_cre
 	}
 
 	spin_lock(&heap->stat_lock);
+	pre_num_of_alloc_bytes = heap->num_of_alloc_bytes;
+	spin_unlock(&heap->stat_lock);
+	trace_ion_heap_grow(heap->name, len, pre_num_of_alloc_bytes);
+
+	spin_lock(&heap->stat_lock);
 	heap->num_of_buffers++;
 	heap->num_of_alloc_bytes += len;
 	if (heap->num_of_alloc_bytes > heap->alloc_bytes_wm)
@@ -74,23 +112,36 @@ static struct ion_buffer *ion_buffer_cre
 
 	INIT_LIST_HEAD(&buffer->attachments);
 	mutex_init(&buffer->lock);
+	mutex_lock(&dev->buffer_lock);
+	ion_buffer_add(dev, buffer);
+	mutex_unlock(&dev->buffer_lock);
 	return buffer;
 
 err1:
 	heap->ops->free(buffer);
 err2:
+	cvi_ion_dump(heap);
 	kfree(buffer);
 	return ERR_PTR(ret);
 }
 
 void ion_buffer_destroy(struct ion_buffer *buffer)
 {
+	u64 pre_num_of_alloc_bytes;
+
 	if (buffer->kmap_cnt > 0) {
 		pr_warn_once("%s: buffer still mapped in the kernel\n",
 			     __func__);
 		buffer->heap->ops->unmap_kernel(buffer->heap, buffer);
 	}
 	buffer->heap->ops->free(buffer);
+
+	spin_lock(&buffer->heap->stat_lock);
+	pre_num_of_alloc_bytes = buffer->heap->num_of_alloc_bytes;
+	spin_unlock(&buffer->heap->stat_lock);
+	trace_ion_heap_shrink(buffer->heap->name, buffer->size,
+			      pre_num_of_alloc_bytes);
+
 	spin_lock(&buffer->heap->stat_lock);
 	buffer->heap->num_of_buffers--;
 	buffer->heap->num_of_alloc_bytes -= buffer->size;
@@ -102,6 +153,11 @@ void ion_buffer_destroy(struct ion_buffe
 static void _ion_buffer_destroy(struct ion_buffer *buffer)
 {
 	struct ion_heap *heap = buffer->heap;
+	struct ion_device *dev = buffer->dev;
+
+	mutex_lock(&dev->buffer_lock);
+	rb_erase(&buffer->node, &dev->buffers);
+	mutex_unlock(&dev->buffer_lock);
 
 	if (heap->flags & ION_HEAP_FLAG_DEFER_FREE)
 		ion_heap_freelist_add(heap, buffer);
@@ -275,6 +331,17 @@ static void ion_dma_buf_release(struct d
 	_ion_buffer_destroy(buffer);
 }
 
+static void *ion_dma_buf_vmap(struct dma_buf *dmabuf)
+{
+	struct ion_buffer *buffer = dmabuf->priv;
+
+	return buffer->vaddr;
+}
+
+static void ion_dma_buf_vunmap(struct dma_buf *dmabuf, void *ptr)
+{
+}
+
 static int ion_dma_buf_begin_cpu_access(struct dma_buf *dmabuf,
 					enum dma_data_direction direction)
 {
@@ -325,6 +392,56 @@ static int ion_dma_buf_end_cpu_access(st
 	return 0;
 }
 
+#ifdef CONFIG_ION_CVITEK
+int ion_buf_begin_cpu_access(struct ion_buffer *buffer)
+{
+	void *vaddr;
+	int ret = 0;
+
+	if (!buffer) {
+		pr_err("%s, buffer is NULL\n", __func__);
+		return -EINVAL;
+	}
+
+	/*
+	 * TODO: Move this elsewhere because we don't always need a vaddr
+	 */
+	if (buffer->heap->ops->map_kernel) {
+		mutex_lock(&buffer->lock);
+		vaddr = ion_buffer_kmap_get(buffer);
+		if (IS_ERR(vaddr)) {
+			ret = PTR_ERR(vaddr);
+			goto unlock;
+		}
+		mutex_unlock(&buffer->lock);
+	}
+
+	mutex_lock(&buffer->lock);
+
+unlock:
+	mutex_unlock(&buffer->lock);
+	return ret;
+}
+EXPORT_SYMBOL(ion_buf_begin_cpu_access);
+
+int ion_buf_end_cpu_access(struct ion_buffer *buffer)
+{
+	if (!buffer) {
+		pr_err("%s, buffer is NULL\n", __func__);
+		return -EINVAL;
+	}
+
+	if (buffer->heap->ops->map_kernel) {
+		mutex_lock(&buffer->lock);
+		ion_buffer_kmap_put(buffer);
+		mutex_unlock(&buffer->lock);
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL(ion_buf_end_cpu_access);
+#endif
+
 static const struct dma_buf_ops dma_buf_ops = {
 	.map_dma_buf = ion_map_dma_buf,
 	.unmap_dma_buf = ion_unmap_dma_buf,
@@ -334,9 +451,93 @@ static const struct dma_buf_ops dma_buf_
 	.detach = ion_dma_buf_detach,
 	.begin_cpu_access = ion_dma_buf_begin_cpu_access,
 	.end_cpu_access = ion_dma_buf_end_cpu_access,
+	.vmap = ion_dma_buf_vmap,
+	.vunmap = ion_dma_buf_vunmap,
 };
 
-static int ion_alloc(size_t len, unsigned int heap_id_mask, unsigned int flags)
+#ifdef CONFIG_ION_CVITEK
+static void *_ion_alloc(size_t len, unsigned int heap_id_mask, unsigned int flags)
+{
+	struct ion_device *dev = internal_dev;
+	struct ion_buffer *buffer;
+	struct ion_heap *heap;
+
+	pr_debug("%s: len %zu heap_id_mask %u flags %x\n", __func__,
+		 len, heap_id_mask, flags);
+	/*
+	 * traverse the list of heaps available in this system in priority
+	 * order.  If the heap type is supported by the client, and matches the
+	 * request of the caller allocate from it.  Repeat until allocate has
+	 * succeeded or all heaps have been tried
+	 */
+	len = PAGE_ALIGN(len);
+
+	if (!len)
+		return ERR_PTR(-EINVAL);
+
+	down_read(&dev->lock);
+	plist_for_each_entry(heap, &dev->heaps, node) {
+		/* if the caller didn't specify this heap id */
+		if (!((1 << heap->id) & heap_id_mask))
+			continue;
+		buffer = ion_buffer_create(heap, dev, len, flags);
+		if (!IS_ERR(buffer))
+			break;
+	}
+	up_read(&dev->lock);
+
+	if (!buffer)
+		return ERR_PTR(-ENODEV);
+
+	return buffer;
+}
+
+struct ion_buffer *
+ion_alloc_nofd(size_t len, unsigned int heap_id_mask, unsigned int flags)
+{
+	return _ion_alloc(len, heap_id_mask, flags);
+}
+
+void ion_free_nofd(struct ion_buffer *buffer)
+{
+	if (buffer)
+		_ion_buffer_destroy(buffer);
+}
+
+int ion_alloc(size_t len, unsigned int heap_id_mask, unsigned int flags, struct ion_buffer **buf)
+{
+	struct ion_buffer *buffer = NULL;
+	DEFINE_DMA_BUF_EXPORT_INFO(exp_info);
+	int fd;
+	struct dma_buf *dmabuf;
+
+	buffer = _ion_alloc(len, heap_id_mask, flags);
+	if (IS_ERR(buffer))
+		return PTR_ERR(buffer);
+
+	exp_info.ops = &dma_buf_ops;
+	exp_info.size = buffer->size;
+	exp_info.flags = O_RDWR;
+	exp_info.priv = buffer;
+
+	dmabuf = dma_buf_export(&exp_info);
+	if (IS_ERR(dmabuf)) {
+		_ion_buffer_destroy(buffer);
+		return PTR_ERR(dmabuf);
+	}
+
+	fd = dma_buf_fd(dmabuf, O_CLOEXEC);
+	if (fd < 0)
+		dma_buf_put(dmabuf);
+
+	if (buffer && buf)
+		*buf = buffer;
+
+	return fd;
+}
+
+#else
+int ion_alloc(size_t len, unsigned int heap_id_mask, unsigned int flags)
 {
 	struct ion_device *dev = internal_dev;
 	struct ion_buffer *buffer = NULL;
@@ -392,8 +593,9 @@ static int ion_alloc(size_t len, unsigne
 
 	return fd;
 }
+#endif
 
-static int ion_query_heaps(struct ion_heap_query *query)
+int ion_query_heaps(struct ion_heap_query *query, int is_kernel)
 {
 	struct ion_device *dev = internal_dev;
 	struct ion_heap_data __user *buffer = u64_to_user_ptr(query->heaps);
@@ -416,14 +618,19 @@ static int ion_query_heaps(struct ion_he
 	max_cnt = query->cnt;
 
 	plist_for_each_entry(heap, &dev->heaps, node) {
-		strncpy(hdata.name, heap->name, MAX_HEAP_NAME);
+		if(heap->name)
+			strncpy(hdata.name, heap->name, MAX_HEAP_NAME);
 		hdata.name[sizeof(hdata.name) - 1] = '\0';
 		hdata.type = heap->type;
 		hdata.heap_id = heap->id;
 
-		if (copy_to_user(&buffer[cnt], &hdata, sizeof(hdata))) {
-			ret = -EFAULT;
-			goto out;
+		if (is_kernel) {
+			buffer[cnt] = hdata;
+		} else {
+			if (copy_to_user(&buffer[cnt], &hdata, sizeof(hdata))) {
+				ret = -EFAULT;
+				goto out;
+			}
 		}
 
 		cnt++;
@@ -441,6 +648,7 @@ out:
 union ion_ioctl_arg {
 	struct ion_allocation_data allocation;
 	struct ion_heap_query query;
+	struct ion_custom_data custom;
 };
 
 static int validate_ioctl_arg(unsigned int cmd, union ion_ioctl_arg *arg)
@@ -463,6 +671,10 @@ static long ion_ioctl(struct file *filp,
 {
 	int ret = 0;
 	union ion_ioctl_arg data;
+	struct ion_device *dev = container_of(filp->private_data, struct ion_device, dev);
+#ifdef CONFIG_ION_CVITEK
+	struct ion_buffer *buffer;
+#endif
 
 	if (_IOC_SIZE(cmd) > sizeof(data))
 		return -EINVAL;
@@ -488,19 +700,55 @@ static long ion_ioctl(struct file *filp,
 	case ION_IOC_ALLOC:
 	{
 		int fd;
+#ifdef CONFIG_ION_CVITEK
+		char *name = vmalloc(MAX_ION_BUFFER_NAME);
 
 		fd = ion_alloc(data.allocation.len,
 			       data.allocation.heap_id_mask,
+			       data.allocation.flags, &buffer);
+#else
+		fd = ion_alloc(data.allocation.len,
+			       data.allocation.heap_id_mask,
 			       data.allocation.flags);
+#endif
 		if (fd < 0)
 			return fd;
 
 		data.allocation.fd = fd;
+#ifdef CONFIG_ION_CVITEK
+		data.allocation.paddr = buffer->paddr;
+		strncpy(name, data.allocation.name, MAX_ION_BUFFER_NAME);
+		buffer->name = name;
+#endif
+		break;
+	}
+	case ION_IOC_ALLOC_LEGACY:
+	{
+		int fd;
+#ifdef CONFIG_ION_CVITEK
+		fd = ion_alloc(data.allocation.len,
+			       data.allocation.heap_id_mask,
+			       data.allocation.flags, &buffer);
+#else
+		fd = ion_alloc(data.allocation.len,
+			       data.allocation.heap_id_mask,
+			       data.allocation.flags);
+#endif
+		if (fd < 0)
+			return fd;
 
+		data.allocation.fd = fd;
+		data.allocation.paddr = buffer->paddr;
 		break;
 	}
 	case ION_IOC_HEAP_QUERY:
-		ret = ion_query_heaps(&data.query);
+		ret = ion_query_heaps(&data.query, false);
+		break;
+	case ION_IOC_CUSTOM:
+		if (!dev->custom_ioctl)
+			return -ENOTTY;
+		ret = dev->custom_ioctl(dev, data.custom.cmd,
+					data.custom.arg);
 		break;
 	default:
 		return -ENOTTY;
@@ -516,7 +764,9 @@ static long ion_ioctl(struct file *filp,
 static const struct file_operations ion_fops = {
 	.owner          = THIS_MODULE,
 	.unlocked_ioctl = ion_ioctl,
-	.compat_ioctl	= compat_ptr_ioctl,
+#ifdef CONFIG_COMPAT
+	.compat_ioctl = compat_ion_ioctl,
+#endif
 };
 
 static int debug_shrink_set(void *data, u64 val)
@@ -614,6 +864,9 @@ void ion_device_add_heap(struct ion_heap
 	 */
 	plist_node_init(&heap->node, -heap->id);
 	plist_add(&heap->node, &dev->heaps);
+#ifdef CONFIG_ION_CVITEK
+	cvi_ion_create_debug_info(heap);
+#endif
 
 	dev->heap_cnt++;
 	up_write(&dev->lock);
@@ -641,9 +894,19 @@ static int ion_device_create(void)
 	}
 
 	idev->debug_root = debugfs_create_dir("ion", NULL);
+	idev->buffers = RB_ROOT;
+	mutex_init(&idev->buffer_lock);
 	init_rwsem(&idev->lock);
 	plist_head_init(&idev->heaps);
 	internal_dev = idev;
 	return 0;
 }
 subsys_initcall(ion_device_create);
+#ifdef CONFIG_ION_CVITEK
+#include <linux/syscalls.h>
+void ion_free(pid_t pid, int fd)
+{
+	ksys_close(fd);
+}
+EXPORT_SYMBOL(ion_free);
+#endif
diff -urpN --no-dereference -X diffgen.ignore test-tree/drivers/staging/android/ion/ion.h milky/drivers/staging/android/ion/ion.h
--- test-tree/drivers/staging/android/ion/ion.h	2024-05-24 09:36:29.343394612 -0400
+++ milky/drivers/staging/android/ion/ion.h	2024-05-21 05:22:27.000000000 -0400
@@ -22,6 +22,29 @@
 #include "../uapi/ion.h"
 
 /**
+ * struct ion_platform_heap - defines a heap in the given platform
+ * @type:	type of the heap from ion_heap_type enum
+ * @id:		unique identifier for heap.  When allocating higher numb ers
+ *		will be allocated from first.  At allocation these are passed
+ *		as a bit mask and therefore can not exceed ION_NUM_HEAP_IDS.
+ * @name:	used for debug purposes
+ * @base:	base address of heap in physical memory if applicable
+ * @size:	size of the heap in bytes if applicable
+ * @priv:	private info passed from the board file
+ *
+ * Provided by the board file.
+ */
+struct ion_platform_heap {
+	enum ion_heap_type type;
+	unsigned int id;
+	const char *name;
+	phys_addr_t base;
+	size_t size;
+	phys_addr_t align;
+	void *priv;
+};
+
+/**
  * struct ion_buffer - metadata for a particular buffer
  * @list:		element in list of deferred freeable buffers
  * @dev:		back pointer to the ion_device
@@ -38,7 +61,10 @@
  * @attachments:	list of devices attached to this buffer
  */
 struct ion_buffer {
-	struct list_head list;
+	union {
+		struct rb_node node;
+		struct list_head list;
+	};
 	struct ion_device *dev;
 	struct ion_heap *heap;
 	unsigned long flags;
@@ -50,6 +76,10 @@ struct ion_buffer {
 	void *vaddr;
 	struct sg_table *sg_table;
 	struct list_head attachments;
+#ifdef CONFIG_ION_CVITEK
+	phys_addr_t paddr;
+	const char *name;
+#endif
 };
 
 void ion_buffer_destroy(struct ion_buffer *buffer);
@@ -57,14 +87,20 @@ void ion_buffer_destroy(struct ion_buffe
 /**
  * struct ion_device - the metadata of the ion device node
  * @dev:		the actual misc device
+ * @buffers:		an rb tree of all the existing buffers
+ * @buffer_lock:	lock protecting the tree of buffers
  * @lock:		rwsem protecting the tree of heaps and clients
  */
 struct ion_device {
 	struct miscdevice dev;
+	struct rb_root buffers;
+	struct mutex buffer_lock;
 	struct rw_semaphore lock;
 	struct plist_head heaps;
 	struct dentry *debug_root;
 	int heap_cnt;
+	long (*custom_ioctl)(struct ion_device *dev, unsigned int cmd,
+			     unsigned long arg);
 };
 
 /**
@@ -152,12 +188,16 @@ struct ion_heap {
 	spinlock_t free_lock;
 	wait_queue_head_t waitqueue;
 	struct task_struct *task;
-
+	struct dentry *heap_dfs_root;
+	u64 total_size;
 	/* heap statistics */
 	u64 num_of_buffers;
 	u64 num_of_alloc_bytes;
 	u64 alloc_bytes_wm;
 
+	int (*debug_show)(struct ion_heap *heap, struct seq_file *s,
+			  void *unused);
+
 	/* protect heap statistics */
 	spinlock_t stat_lock;
 };
@@ -177,7 +217,17 @@ void ion_heap_unmap_kernel(struct ion_he
 int ion_heap_map_user(struct ion_heap *heap, struct ion_buffer *buffer,
 		      struct vm_area_struct *vma);
 int ion_heap_buffer_zero(struct ion_buffer *buffer);
+int ion_heap_pages_zero(struct page *page, size_t size, pgprot_t pgprot);
 
+#ifdef CONFIG_ION_CVITEK
+int ion_alloc(size_t len,
+	      unsigned int heap_id_mask,
+	      unsigned int flags, struct ion_buffer **buf);
+#else
+int ion_alloc(size_t len,
+	      unsigned int heap_id_mask,
+	      unsigned int flags);
+#endif
 /**
  * ion_heap_init_shrinker
  * @heap:		the heap
@@ -289,6 +339,15 @@ void ion_page_pool_destroy(struct ion_pa
 struct page *ion_page_pool_alloc(struct ion_page_pool *pool);
 void ion_page_pool_free(struct ion_page_pool *pool, struct page *page);
 
+#ifdef CONFIG_ION_CARVEOUT_HEAP
+struct ion_heap *ion_carveout_heap_create(struct ion_platform_heap *heap_data);
+#else
+static inline struct ion_heap
+*ion_carveout_heap_create(struct ion_platform_heap *heap_data)
+{
+	return ERR_PTR(-EINVAL);
+}
+#endif
 /** ion_page_pool_shrink - shrinks the size of the memory cached in the pool
  * @pool:		the pool
  * @gfp_mask:		the memory type to reclaim
@@ -299,4 +358,34 @@ void ion_page_pool_free(struct ion_page_
 int ion_page_pool_shrink(struct ion_page_pool *pool, gfp_t gfp_mask,
 			 int nr_to_scan);
 
+int ion_query_heaps(struct ion_heap_query *query, int is_kernel);
+
+#ifdef CONFIG_ION_CVITEK
+struct ion_buffer *
+ion_alloc_nofd(size_t len, unsigned int heap_id_mask, unsigned int flags);
+void ion_free(pid_t fd_pid, int fd);
+void ion_free_nofd(struct ion_buffer *buffer);
+int ion_buf_begin_cpu_access(struct ion_buffer *buffer);
+int ion_buf_end_cpu_access(struct ion_buffer *buffer);
+
+#ifdef CONFIG_ION_CARVEOUT_HEAP
+struct ion_heap *ion_carveout_heap_create(struct ion_platform_heap *heap_data);
+#else
+static inline struct ion_heap
+*ion_carveout_heap_create(struct ion_platform_heap *heap_data)
+{
+	return ERR_PTR(-EINVAL);
+}
+#endif
+#endif
+#ifdef CONFIG_ION_CHUNK_HEAP
+struct ion_heap
+*ion_chunk_heap_create(struct ion_platform_heap *data, u32 chunk_size);
+#else
+static inline struct ion_heap
+*ion_chunk_heap_create(struct ion_platform_heap *data, u32 chunk_size)
+{
+	return ERR_PTR(-EINVAL);
+}
+#endif
 #endif /* _ION_H */
diff -urpN --no-dereference -X diffgen.ignore test-tree/drivers/staging/android/ion/ion_heap.c milky/drivers/staging/android/ion/ion_heap.c
--- test-tree/drivers/staging/android/ion/ion_heap.c	2024-05-24 09:36:29.343394612 -0400
+++ milky/drivers/staging/android/ion/ion_heap.c	2024-05-21 05:22:27.000000000 -0400
@@ -14,39 +14,25 @@
 #include <uapi/linux/sched/types.h>
 #include <linux/scatterlist.h>
 #include <linux/vmalloc.h>
-
 #include "ion.h"
+#include <linux/io.h>
 
 void *ion_heap_map_kernel(struct ion_heap *heap,
 			  struct ion_buffer *buffer)
 {
-	struct sg_page_iter piter;
 	void *vaddr;
-	pgprot_t pgprot;
-	struct sg_table *table = buffer->sg_table;
-	int npages = PAGE_ALIGN(buffer->size) / PAGE_SIZE;
-	struct page **pages = vmalloc(array_size(npages,
-						 sizeof(struct page *)));
-	struct page **tmp = pages;
 
-	if (!pages)
-		return ERR_PTR(-ENOMEM);
+	pr_debug("ion_heap_map_kernel addr=0x%llx, size=%lu\n", buffer->paddr, PAGE_ALIGN(buffer->size));
 
 	if (buffer->flags & ION_FLAG_CACHED)
-		pgprot = PAGE_KERNEL;
+		vaddr = memremap(buffer->paddr, PAGE_ALIGN(buffer->size), MEMREMAP_WB);
 	else
-		pgprot = pgprot_writecombine(PAGE_KERNEL);
-
-	for_each_sgtable_page(table, &piter, 0) {
-		BUG_ON(tmp - pages >= npages);
-		*tmp++ = sg_page_iter_page(&piter);
-	}
-
-	vaddr = vmap(pages, npages, VM_MAP, pgprot);
-	vfree(pages);
+		vaddr = ioremap(buffer->paddr, PAGE_ALIGN(buffer->size));
 
-	if (!vaddr)
+	if (!vaddr) {
+		pr_err("ion_heap_map_kernel map failed\n");
 		return ERR_PTR(-ENOMEM);
+	}
 
 	return vaddr;
 }
@@ -54,7 +40,10 @@ void *ion_heap_map_kernel(struct ion_hea
 void ion_heap_unmap_kernel(struct ion_heap *heap,
 			   struct ion_buffer *buffer)
 {
-	vunmap(buffer->vaddr);
+	if (buffer->flags & ION_FLAG_CACHED)
+		memunmap(buffer->vaddr);
+	else
+		iounmap(buffer->vaddr);
 }
 
 int ion_heap_map_user(struct ion_heap *heap, struct ion_buffer *buffer,
@@ -127,6 +116,21 @@ int ion_heap_buffer_zero(struct ion_buff
 	return ion_heap_sglist_zero(table, pgprot);
 }
 
+int ion_heap_pages_zero(struct page *page, size_t size, pgprot_t pgprot)
+{
+	struct scatterlist sg;
+	struct sg_table sgt;
+
+	sg_init_table(&sg, 1);
+	sg_set_page(&sg, page, size, 0);
+
+	sgt.sgl = &sg;
+	sgt.nents = 1;
+	sgt.orig_nents = 1;
+
+	return ion_heap_sglist_zero(&sgt, pgprot);
+}
+
 void ion_heap_freelist_add(struct ion_heap *heap, struct ion_buffer *buffer)
 {
 	spin_lock(&heap->free_lock);
diff -urpN --no-dereference -X diffgen.ignore test-tree/drivers/staging/android/ion/ion_system_heap.c milky/drivers/staging/android/ion/ion_system_heap.c
--- test-tree/drivers/staging/android/ion/ion_system_heap.c	2024-05-24 09:36:29.343394612 -0400
+++ milky/drivers/staging/android/ion/ion_system_heap.c	2024-05-21 05:22:27.000000000 -0400
@@ -11,6 +11,7 @@
 #include <linux/highmem.h>
 #include <linux/mm.h>
 #include <linux/scatterlist.h>
+#include <linux/seq_file.h>
 #include <linux/slab.h>
 #include <linux/vmalloc.h>
 
@@ -212,6 +213,29 @@ static struct ion_heap_ops system_heap_o
 	.shrink = ion_system_heap_shrink,
 };
 
+static int ion_system_heap_debug_show(struct ion_heap *heap, struct seq_file *s,
+				      void *unused)
+{
+	struct ion_system_heap *sys_heap = container_of(heap,
+							struct ion_system_heap,
+							heap);
+	int i;
+	struct ion_page_pool *pool;
+
+	for (i = 0; i < NUM_ORDERS; i++) {
+		pool = sys_heap->pools[i];
+
+		seq_printf(s, "%d order %u highmem pages %lu total\n",
+			   pool->high_count, pool->order,
+			   (PAGE_SIZE << pool->order) * pool->high_count);
+		seq_printf(s, "%d order %u lowmem pages %lu total\n",
+			   pool->low_count, pool->order,
+			   (PAGE_SIZE << pool->order) * pool->low_count);
+	}
+
+	return 0;
+}
+
 static void ion_system_heap_destroy_pools(struct ion_page_pool **pools)
 {
 	int i;
@@ -259,6 +283,7 @@ static struct ion_heap *__ion_system_hea
 	if (ion_system_heap_create_pools(heap->pools))
 		goto free_heap;
 
+	heap->heap.debug_show = ion_system_heap_debug_show;
 	return &heap->heap;
 
 free_heap:
diff -urpN --no-dereference -X diffgen.ignore test-tree/drivers/staging/android/uapi/ion.h milky/drivers/staging/android/uapi/ion.h
--- test-tree/drivers/staging/android/uapi/ion.h	2024-05-24 09:36:29.343394612 -0400
+++ milky/drivers/staging/android/uapi/ion.h	2024-05-21 05:22:27.000000000 -0400
@@ -56,6 +56,8 @@ enum ion_heap_type {
  *
  */
 
+#define MAX_ION_BUFFER_NAME 32
+
 /**
  * struct ion_allocation_data - metadata passed from userspace for allocations
  * @len:		size of the allocation
@@ -72,6 +74,17 @@ struct ion_allocation_data {
 	__u32 flags;
 	__u32 fd;
 	__u32 unused;
+	__u64 paddr;
+	char name[MAX_ION_BUFFER_NAME];
+};
+
+struct ion_allocation_data_legacy {
+	__u64 len;
+	__u32 heap_id_mask;
+	__u32 flags;
+	__u32 fd;
+	__u32 unused;
+	__u64 paddr;
 };
 
 #define MAX_HEAP_NAME			32
@@ -104,6 +117,19 @@ struct ion_heap_query {
 	__u32 reserved2;
 };
 
+/**
+ * struct ion_custom_data - metadata passed to/from userspace for a custom ioctl
+ * @cmd:	the custom ioctl function to call
+ * @arg:	additional data to pass to the custom ioctl, typically a user
+ *		pointer to a predefined structure
+ *
+ * This works just like the regular cmd and arg fields of an ioctl.
+ */
+struct ion_custom_data {
+	unsigned int cmd;
+	unsigned long arg;
+};
+
 #define ION_IOC_MAGIC		'I'
 
 /**
@@ -115,6 +141,17 @@ struct ion_heap_query {
 #define ION_IOC_ALLOC		_IOWR(ION_IOC_MAGIC, 0, \
 				      struct ion_allocation_data)
 
+#define ION_IOC_ALLOC_LEGACY		_IOWR(ION_IOC_MAGIC, 0, \
+				      struct ion_allocation_data_legacy)
+
+/**
+ * DOC: ION_IOC_CUSTOM - call architecture specific ion ioctl
+ *
+ * Takes the argument of the architecture specific ioctl to call and
+ * passes appropriate userdata for that ioctl
+ */
+#define ION_IOC_CUSTOM		_IOWR(ION_IOC_MAGIC, 6, struct ion_custom_data)
+
 /**
  * DOC: ION_IOC_HEAP_QUERY - information about available heaps
  *
